# Small mlfow pipeline

## Discalmer
Данный мини пет-проект создан, чтобы продемонстрировать владение базовыми навыками MLFlow, DVC и Apache Airflow в контексте MLOps. Для демонстрации экспериментов и логирования взяты модели и параметры из другого моего пет-проекта (вставить ссылку) - за более подробным исследованием датасета и моделей следует перейти туда.

## Краткое описание выбранного датасета и задачи 
Для проекта я выбрал датасет по [проектам Kaggle](https://www.kaggle.com/datasets/codename007/funding-successful-projects). Задача состоит в бинарной классификации успеха проекта по различным признакам. В данном проекте не будет приведен EDA, feature engineering, подбор, оптимизация моделей и тд - всё это можно посмотреть в оригинальном проекте по ссылке выше.

## Описание использованных алгоритмов и технологий
- В данном проекте бинарная классификаиця строится с помощью двух моделей - логистической регресии и LightGBM бустинга. Также проводится несколько экспериментов для сравнения метрик для разных гиперпараметров/моделей.
- Метрики экспериментов логируются с помощью MLFlow локально.
- Изменения в проекте и данных отслеживаются с помощью git и dvc.

## Инструкции по запуску проекта
1) Установить нужные библиотеки из requirements.txt.
2) Установить нужный софт для LightGBM, прописав команду (Ubuntu / Debian / WSL с Ubuntu) `sudo apt install -y libgomp1`

## Команды для воспроизведения экспериментов
1) Через DVC: `dvc repro --force`
2) Через Airflow:
  - Установливаем переменную AIRFLOW_HOME `export AIRFLOW_HOME=/путь/к/моему/проекту/airflow`
  - Запускаем в bash `airflow standalone`
  - Входим под предоставленной учеткой на localhost
  - Запускаем эксперимент

**Disclaimer:** если запускать проект через WSL, то вероятно будут проблемы с доступом в Airflow. Решение проблемы замороченное, поэтому здесь я его не описываю.



## Результаты экспериментов и выводы
Были протестированы и сопоставлены разные конфигурации моделей:
1) Для логистической регрессии:
  - baseline (дефолтные параметры)
  - оптимальная из pet-проекта
  - вариации оптимальной с высокой и низкой регуляризацией
2) Для бустинга
  - baseline (дефолтные параметры)
  - оптимальная из pet-проекта
  - вариация оптимальной с без учёта дисбаланса классов

Закономерно оптимальная модель показала наилучшие f1-score и PR AUC.


